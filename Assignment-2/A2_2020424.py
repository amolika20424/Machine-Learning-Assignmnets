# -*- coding: utf-8 -*-
"""Copy of A1_2020424_Report.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/158aK6ebYsCVi215kDEP45vchoY-RPZSw

#3c: Linear Regression
"""

import pandas as pd 
from sklearn import datasets
from sklearn import linear_model
from sklearn import model_selection
from sklearn.model_selection import train_test_split
# Load the dataset
from sklearn.model_selection import KFold
diabetes = datasets.load_diabetes()
from matplotlib import pyplot as plt
import numpy as np

diabetes = datasets.load_diabetes(as_frame=True)

print(type(diabetes['data']))

data = pd.read_csv("/content/drive/MyDrive/Datasets/diabetes-dataset.csv")

data

df_max_scaled = data.copy()
  
# apply normalization techniques
for column in df_max_scaled.columns:
    df_max_scaled[column] = df_max_scaled[column]  / df_max_scaled[column].abs().max()

df_max_scaled

data = df_max_scaled
train, test = train_test_split(data, test_size=0.1)

import math
class Linear_Regression:
    # Initiliase the class with user defined learning rate and number of epochs
    def __init__(self, learning_rate, n_epochs):
        self.learning_rate = learning_rate
        self.n_epochs = n_epochs
 
 
    def gradient_descent(self,x,y):
        theta_curr = np.zeros(x.shape[1]) #weights
        bias = 0 #bias
        n_samples = y.size
        cost_l = [0] * self.n_epochs
        rmse = [0] * self.n_epochs
 
        for i in range(self.n_epochs):
            y_pred = x.dot(theta_curr) + bias
 
            #computing gradients
            dm = (-2/n_samples)*x.T.dot(y-y_pred)
            dc = (-2/n_samples)*np.sum(y-y_pred)
 
            #Updating weights
            theta_curr = theta_curr - self.learning_rate*dm
            bias = bias -  self.learning_rate*dc
 
            #Calculating the cost associated with each iteration
            cost = (1/(2*n_samples))*np.sum((y_pred-y)**2)
            cost_l[i] = cost 
            mse = ((1/(n_samples))*np.sum((y_pred-y)**2))
            mse = math.sqrt(mse)
            rmse[i] = mse
        return theta_curr, bias, cost_l, rmse
    
    def predict(self,X,theta,bias):
        return X.dot(theta) + bias

model = Linear_Regression(learning_rate=0.01,n_epochs=200)
Y= train['Outcome']
X= train.drop(['Outcome'],axis=1)

X=X.to_numpy()
Y=Y.to_numpy()

kf = KFold(n_splits=3)

index =0
rmse_sum =[]
t_min=[]

for train_index, test_index in kf.split(train):
    X_train, X_test = X[train_index], X[test_index] 
    Y_train, Y_test = Y[train_index], Y[test_index]
    
    #t: Theta obtained after gradient descent, b: Bias obtained after gradient desecnt, c: cost value
    t,b,c,rmse = model.gradient_descent(X_train,Y_train)
    y_pred_train = X_train.dot(t)+b
    y_pred = model.predict(X_test,t,b) #validation set
#     fig, axes = plt.subplots(1, 3)
#     axes[0].plot(rmse,'g') 
    plt.plot(rmse)
    t_len = len(t)
    t_min.extend(t)
    rmse_sum.append(sum(rmse))
min_ind = rmse_sum.index(min(rmse_sum))
t_best = t_min[min_ind*t_len:(min_ind*t_len+t_len)]

"""Performed 3 fold k cross validation to obtain better results"""

Y_test = test['Outcome']
X_test = test.drop(['Outcome'],axis=1)
Y_test = Y_test.to_numpy()
X_test = X_test.to_numpy()

y_pred_test = X_test.dot(t_best)

t,b,c,rmse = model.gradient_descent(X_test,Y_test)
y_pred_train = X_test.dot(t_best)+b
y_pred = model.predict(X_test,t_best,b) 
plt.plot(rmse)

print("RMSE using Linear Regression from Scratch: ",min(rmse))

from sklearn.linear_model import LinearRegression
from sklearn import metrics
regr = LinearRegression()
rmse_sci= []
t_reg = []
kfold_datasets_stored=[]

for train_index, test_index in kf.split(train):
    X_train, X_test = X[train_index], X[test_index] 
    Y_train, Y_test = Y[train_index], Y[test_index]
    kfold_datasets_stored.append([X_train, X_test,Y_train, Y_test])
    regr = LinearRegression()
    regr.fit(X_train, Y_train)
    t_reg.extend(regr.coef_)
    y_pred = regr.predict(X_test)
    rmse_sci.append(np.sqrt(metrics.mean_squared_error(Y_test,y_pred)))

min_index = np.argmin(np.array(rmse_sci))
X_train_kf, X_test_kf, Y_train_kf, Y_test_kf= kfold_datasets_stored[min_index]
model=LinearRegression()
model.fit(X_train_kf,Y_train_kf)
y_pred_kf= model.predict(X_test_kf)
np.sqrt(metrics.mean_squared_error(Y_test_kf,y_pred_kf))
rmse_sci = np.sqrt(metrics.mean_squared_error(Y_test_kf,y_pred_kf))
print("RMSE using Linear Regression (SciKit Learn): ", rmse_sci)

"""RMSE using Linear Regression (SciKit Learn) is 0.3974252185078301

RMSE using Linear Regression from Scratch is 0.4552361135405836

"""